MobileViT(
  (conv_1): ConvLayer(
    (block): Sequential(
      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): ReLU()
    )
  )
  (layer_1): Sequential(
    (0): InvertedResidual(
      (block): Sequential(
        (exp_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (conv_3x3): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (red_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer_2): Sequential(
    (0): InvertedResidual(
      (block): Sequential(
        (exp_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (conv_3x3): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (red_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): InvertedResidual(
      (block): Sequential(
        (exp_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (conv_3x3): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (red_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): InvertedResidual(
      (block): Sequential(
        (exp_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (conv_3x3): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (red_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer_3): Sequential(
    (0): InvertedResidual(
      (block): Sequential(
        (exp_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (conv_3x3): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (red_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): MobileViTBlock(
      (local_rep): Sequential(
        (conv_3x3): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (conv_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
      (global_rep): Sequential(
        (0): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer()
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer()
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer()
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer()
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer()
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer()
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
      )
      (conv_proj): ConvLayer(
        (block): Sequential(
          (conv): Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
      )
      (fusion): ConvLayer(
        (block): Sequential(
          (conv): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
      )
    )
  )
  (layer_4): Sequential(
    (0): InvertedResidual(
      (block): Sequential(
        (exp_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (conv_3x3): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (red_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): MobileViTBlock(
      (local_rep): Sequential(
        (conv_3x3): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (conv_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
      (global_rep): Sequential(
        (0): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer()
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer()
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer()
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer()
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer()
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer()
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer()
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer()
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer()
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer()
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer()
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer()
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (conv_proj): ConvLayer(
        (block): Sequential(
          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
      )
      (fusion): ConvLayer(
        (block): Sequential(
          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
      )
    )
  )
  (layer_5): Sequential(
    (0): InvertedResidual(
      (block): Sequential(
        (exp_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (conv_3x3): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (red_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): MobileViTBlock(
      (local_rep): Sequential(
        (conv_3x3): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU()
          )
        )
        (conv_1x1): ConvLayer(
          (block): Sequential(
            (conv): Conv2d(160, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
      (global_rep): Sequential(
        (0): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer()
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer()
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer()
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer()
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer()
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer()
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer()
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer()
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer()
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer()
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
      )
      (conv_proj): ConvLayer(
        (block): Sequential(
          (conv): Conv2d(240, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
      )
      (fusion): ConvLayer(
        (block): Sequential(
          (conv): Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
      )
    )
  )
  (conv_1x1_exp): ConvLayer(
    (block): Sequential(
      (conv): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): ReLU()
    )
  )
  (classifier): Sequential(
    (global_pool): GlobalPool()
    (fc): LinearLayer()
  )
)
