MobileViT(
  (conv_1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
  (layer_1): Sequential(
    (0): InvertedResidual(in_channels=16, out_channels=16, stride=1, exp=2, dilation=1)
  )
  (layer_2): Sequential(
    (0): InvertedResidual(in_channels=16, out_channels=24, stride=2, exp=2, dilation=1)
    (1): InvertedResidual(in_channels=24, out_channels=24, stride=1, exp=2, dilation=1)
    (2): InvertedResidual(in_channels=24, out_channels=24, stride=1, exp=2, dilation=1)
  )
  (layer_3): Sequential(
    (0): InvertedResidual(in_channels=24, out_channels=48, stride=2, exp=2, dilation=1)
    (1): MobileViTBlock(
      (local_rep): Sequential(
        (conv_3x3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
        (conv_1x1): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, bias=False)
      )
      (global_rep): Sequential(
        (0): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer(in_features=64, out_features=192, bias=True)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer(in_features=64, out_features=64, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer(in_features=64, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer(in_features=128, out_features=64, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer(in_features=64, out_features=192, bias=True)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer(in_features=64, out_features=64, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer(in_features=64, out_features=128, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer(in_features=128, out_features=64, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (conv_proj): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
      (fusion): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
    )
  )
  (layer_4): Sequential(
    (0): InvertedResidual(in_channels=48, out_channels=64, stride=2, exp=2, dilation=1)
    (1): MobileViTBlock(
      (local_rep): Sequential(
        (conv_3x3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
        (conv_1x1): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False, bias=False)
      )
      (global_rep): Sequential(
        (0): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer(in_features=80, out_features=240, bias=True)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer(in_features=80, out_features=80, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer(in_features=80, out_features=160, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer(in_features=160, out_features=80, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer(in_features=80, out_features=240, bias=True)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer(in_features=80, out_features=80, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer(in_features=80, out_features=160, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer(in_features=160, out_features=80, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer(in_features=80, out_features=240, bias=True)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer(in_features=80, out_features=80, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer(in_features=80, out_features=160, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer(in_features=160, out_features=80, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer(in_features=80, out_features=240, bias=True)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer(in_features=80, out_features=80, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer(in_features=80, out_features=160, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer(in_features=160, out_features=80, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
      )
      (conv_proj): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
      (fusion): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
    )
  )
  (layer_5): Sequential(
    (0): InvertedResidual(in_channels=64, out_channels=80, stride=2, exp=2, dilation=1)
    (1): MobileViTBlock(
      (local_rep): Sequential(
        (conv_3x3): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
        (conv_1x1): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False, bias=False)
      )
      (global_rep): Sequential(
        (0): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer(in_features=96, out_features=288, bias=True)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer(in_features=96, out_features=96, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer(in_features=96, out_features=192, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer(in_features=192, out_features=96, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer(in_features=96, out_features=288, bias=True)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer(in_features=96, out_features=96, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer(in_features=96, out_features=192, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer(in_features=192, out_features=96, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (qkv_proj): LinearLayer(in_features=96, out_features=288, bias=True)
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (out_proj): LinearLayer(in_features=96, out_features=96, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (2): Dropout(p=0.1, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): LinearLayer(in_features=96, out_features=192, bias=True)
            (2): ReLU()
            (3): Dropout(p=0.0, inplace=False)
            (4): LinearLayer(in_features=192, out_features=96, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (conv_proj): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
      (fusion): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
    )
  )
  (conv_1x1_exp): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False, normalization=BatchNorm2d, activation=PReLU, bias=False)
  (classifier): Sequential(
    (global_pool): GlobalPool(type=mean)
    (dropout): Dropout(p=0.2, inplace=True)
    (fc): LinearLayer(in_features=320, out_features=1000, bias=True)
  )
)
